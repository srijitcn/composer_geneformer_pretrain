name: multi-gpu-test
image: mosaicml/pytorch:2.2.2_cu121-python3.11-ubuntu20.04

integrations:
- integration_type: git_repo
  git_repo: srijitcn/composer_geneformer_pretrain
  git_branch: main # use your branch
  ssh_clone: true # Should be true if using a private repo

command: |

  echo "AWS_ACCESS_KEY_ID : $AWS_ACCESS_KEY_ID"
  echo "AWS_SECRET_ACCESS_KEY : $AWS_SECRET_ACCESS_KEY"

  apt update
  apt install unzip
  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
  unzip awscliv2.zip
  sudo ./aws/install

  cd composer_geneformer_pretrain
  sh ./geneformer_prep.sh 
  pip install -r requirements.txt

  cd /Geneformer/
  aws s3 cp s3://srijit-nair-sandbox-bucket/geneformer/data/ ./data
  
  composer -n 8 train.py /mnt/config/parameters.yaml

compute:
  gpus: 8
  cluster: r8z11

parameters:
  model_config:
    hidden_size: 256
    num_hidden_layers: 6
    initializer_range: 0.02
    layer_norm_eps: 1e-12
    attention_probs_dropout_prob: 0.02
    hidden_dropout_prob: 0.02
    intermediate_size: 512
    hidden_act: relu
    max_position_embeddings: 2048
    model_type: bert
    num_attention_heads: 4
  
  fsdp_config:
    sharding_strategy: FULL_SHARD,
    state_dict_type: sharded,
    cpu_offload: False, # Not supported yet
    mixed_precision: DEFAULT,
    backward_prefetch: BACKWARD_POST,
    activation_checkpointing: False,
    activation_cpu_offload: False,
    verbose: True

  # Optimization
  scheduler:
    name: linear_decay_with_warmup
    t_warmup: 0.06dur # Warmup to the full LR for 6% of the training duration
    alpha_f: 0.02 # Linearly decay to 0.02x the full LR by the end of the training duration

  optimizer:
    name: decoupled_adamw
    lr: 5.0e-4 # Peak learning rate
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-06
    weight_decay: 1.0e-5 # Amount of weight decay regularization

  callbacks:
    speed_monitor:
      window_size: 500
    lr_monitor: {}

  #make sure you have configured databricks secret
  loggers:
    mlflow:
      tracking_uri: databricks
      experiment_name: mlflow_experiments/geneformer_pretraining

  seed_val : 42
  run_name : geneformer_pretrain
  working_dir : /pretrain/temp
  # Streaming Dataset
  data_bucket_name : srijit-nair-sandbox-bucket
  data_bucket_key : geneformer/data
  token_dictionary_filename : token_dictionary.pkl
  mlm_probability: 0.15
  # Batching
  train_batch_size: 12
  eval_batch_size: 12
  device_train_microbatch_size: auto
  precision: fp16
  # Logging
  progress_bar: false
  log_to_console: true
  console_log_interval: 1ba
  eval_interval: 2000ba
  global_train_batch_size: 4096
  #Checkpoint to remote object store
  save_interval: 5ep
  save_num_checkpoints_to_keep: 1  # Important, this cleans up checkpoints saved to DISK
  save_folder: "dbfs:/databricks/mlflow-tracking/{mlflow_experiment_id}/{mlflow_run_id}/artifacts/"
